{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d6e2700a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<requests_oauthlib.oauth1_session.OAuth1Session object at 0x7f8b6f108400>\n",
      "投稿準備完了\n",
      "スニダンのページの取得完了\n",
      "画像の取得完了\n",
      "ここから詳細ページ\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36m<cell line: 78>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     76\u001b[0m soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(r_text, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     78\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m soup_text[:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m【リーク】\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 79\u001b[0m     soup_h1\u001b[38;5;241m=\u001b[39m\u001b[43msoup\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfind_all\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mh1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpage-title\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m抽選/定価/販売店舗まとめ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m【リーク】\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     80\u001b[0m     soup_text\u001b[38;5;241m=\u001b[39msoup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdiv\u001b[39m\u001b[38;5;124m\"\u001b[39m,attrs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle-content\u001b[39m\u001b[38;5;124m\"\u001b[39m})[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mtext\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     81\u001b[0m     pos1\u001b[38;5;241m=\u001b[39msoup_text\u001b[38;5;241m.\u001b[39mfind(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mについて\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import json\n",
    "import sys\n",
    "sys.path.append(\"/Users/kuramochiosuke/.pyenv/versions/3.10.4/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages\")\n",
    "from requests_oauthlib import OAuth1Session\n",
    "sys.path.append(\"/Users/kuramochiosuke/.pyenv/versions/3.10.4/lib/python3.10/site-packages\")\n",
    "from dotenv import find_dotenv, load_dotenv\n",
    "import requests\n",
    "import schedule\n",
    "import time\n",
    "import openpyxl\n",
    "\n",
    "\n",
    "\n",
    "# .envファイルを探して読み込み\n",
    "env_file = find_dotenv()\n",
    "load_dotenv(env_file)  \n",
    "\n",
    "CONSUMER_KEY = os.environ.get('CONSUMER_KEY')\n",
    "CONSUMER_SECRET = os.environ.get('CONSUMER_SECRET')\n",
    "ACCESS_KEY = os.environ.get('ACCESS_KEY')\n",
    "ACCESS_KEY_SECRET = os.environ.get('ACCESS_KEY_SECRET')\n",
    "\n",
    "# Twitterの認証\n",
    "twitter = OAuth1Session(CONSUMER_KEY, CONSUMER_SECRET, ACCESS_KEY, ACCESS_KEY_SECRET)\n",
    "print(twitter)\n",
    "\n",
    "#エンドポイント\n",
    "url_text = 'https://api.twitter.com/1.1/statuses/update.json'\n",
    "url_media = \"https://upload.twitter.com/1.1/media/upload.json\"\n",
    "\n",
    "\n",
    "\n",
    "# ここまでTwitter投稿の準備\n",
    "print(\"投稿準備完了\")\n",
    "\n",
    "\n",
    "#スニだんのページの指定\n",
    "URL = 'https://snkrdunk.com'\n",
    "# リクエストヘッダの指定\n",
    "headers = {\"User-Agent\": \"hoge\"}\n",
    "response = requests.get(URL,  headers=headers)\n",
    "r_text=response.text\n",
    "soup = BeautifulSoup(r_text, 'html.parser')\n",
    "\n",
    "\n",
    "print('スニダンのページの取得完了')\n",
    "\n",
    "\n",
    "\n",
    "# 記事の取得 \n",
    "soup_article=soup.find_all(\"article\",attrs={\"class\",\"article-list new\"})[0]      \n",
    "soup_text=soup_article.find_all(\"h3\")[0].find(\"a\").text.replace(\"\\n\",\"\").replace(\"\\t\",\"\").replace(\"定価/\",\"\")\n",
    "# 画像の取得\n",
    "soup_img=soup_article.find_all(\"img\")[0]['src']\n",
    "# 詳細ページのリンクを取得\n",
    "soup_url=\"https://snkrdunk.com/\"+soup_article.find(\"a\")['href']\n",
    "# 画像の処理\n",
    "response = requests.get(soup_img)\n",
    "image = response.content\n",
    "files = {\"media\" : image}\n",
    "req_media = twitter.post(url_media, files = files)\n",
    "media_id = json.loads(req_media.text)['media_id']\n",
    "print('画像の取得完了')\n",
    "\n",
    "\n",
    "print(\"ここから詳細ページ\")\n",
    "\n",
    "URL = soup_url\n",
    "# リクエストヘッダの指定\n",
    "headers = {\"User-Agent\": \"hoge\"}\n",
    "response = requests.get(URL,  headers=headers)\n",
    "r_text=response.text\n",
    "soup = BeautifulSoup(r_text, 'html.parser')\n",
    "\n",
    "if soup_text[:5]==\"【リーク】\":\n",
    "    soup_h1=soup.find_all(\"h1\",attrs={\"class\",\"page-title\"})[0].text.replace(\"抽選/定価/販売店舗まとめ\",\"\").replace(\"【リーク】\",\"\")\n",
    "    soup_text=soup.find_all(\"div\",attrs={\"class\",\"article-content\"})[0].text.replace(\"\\n\",\"\").replace(\"\\t\",\"\")\n",
    "    pos1=soup_text.find(\"について\")\n",
    "    text1=soup_text[pos1+4:]\n",
    "    pos2=text1.find(\"発売予定！\")\n",
    "    if pos2<10: \n",
    "        pos3=text1.find(\"発売！\")\n",
    "        if pos3<10:\n",
    "            pos4=text1.find(\"復刻予定！\")\n",
    "            if pos4<10:\n",
    "                pos5=text1.find(\"リリース！\")\n",
    "                if pos5<10:\n",
    "                    pos6=text1.find(\"リリース予定！\")\n",
    "                    soup_cap=text1[:pos6+7]\n",
    "                    params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "                else:\n",
    "                    soup_cap=text1[:pos5+5]\n",
    "                    params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "            else:\n",
    "                soup_cap=text1[:pos4+5]\n",
    "                params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "        else:\n",
    "            soup_cap=text1[:pos3+3]\n",
    "            params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "    else:\n",
    "        soup_cap=text1[:pos2+5]\n",
    "        params = {'status':\"リーク情報!!!\\n\\n{}\\n\\n{}\\n\\n情報が入り次第更新!!!\".format(soup_h1,soup_cap),'media_ids':[media_id]}\n",
    "    wb = openpyxl.load_workbook('sneaker.xlsx')\n",
    "    ws = wb[\"Sheet1\"]\n",
    "    for i in range(wb['Sheet1'].max_row):\n",
    "        if ws.cell(row=i+1,column=1).value==params[\"status\"]:\n",
    "            print(\"投稿済みです\")\n",
    "            break    \n",
    "        elif i==wb['Sheet1'].max_row-1:  \n",
    "            ws.cell(row=wb['Sheet1'].max_row+1,column=1).value = params[\"status\"]\n",
    "            wb.save('sneaker.xlsx')\n",
    "            print(\"保存しました\")\n",
    "            twitter.post(url_text, params = params)\n",
    "            print(\"投稿しました\")  \n",
    "    print(\"\")\n",
    "elif soup_text[:9]==\"【販売リンクあり】\":\n",
    "    a_count=len(soup.find_all(\"div\",attrs={\"class\",\"sneaker-release-shop-box pre-release\"})[0].find_all(\"a\"))\n",
    "    for i in range(a_count):\n",
    "        soup_block=soup.find_all(\"div\",attrs={\"class\",\"sneaker-release-shop-box\"})[0].find_all(\"a\")[i]\n",
    "        soup_link=soup_block['href']\n",
    "        soup_app_name=soup_block.find_all(\"div\",attrs={\"class\",\"left-box\"})[0].text\n",
    "        soup_data=soup_block.find_all(\"div\",attrs={\"class\",\"shop-right-box\"})[0].text\n",
    "        params = {'status': \"{}\\n\\n{}  {}\\n{}\\n\".format(soup_text,soup_app_name,soup_data,soup_link),'media_ids':[media_id]}\n",
    "        wb = openpyxl.load_workbook('sneaker.xlsx')\n",
    "        ws = wb[\"Sheet1\"]\n",
    "        for i in range(wb['Sheet1'].max_row):\n",
    "            if ws.cell(row=i+1,column=1).value==params[\"status\"]:\n",
    "                print(\"投稿済みです\")\n",
    "                break    \n",
    "            elif i==wb['Sheet1'].max_row-1:   \n",
    "                ws.cell(row=wb['Sheet1'].max_row+1,column=1).value = params[\"status\"]\n",
    "                wb.save('sneaker.xlsx')\n",
    "                print(\"保存しました\")\n",
    "                twitter.post(url_text, params = params)\n",
    "                print(\"投稿しました\")  \n",
    "        print(\"\")\n",
    "else:\n",
    "    print(\"除外\")\n",
    "    print(\"\")\n",
    "print(\"処理終了\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af8db964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
